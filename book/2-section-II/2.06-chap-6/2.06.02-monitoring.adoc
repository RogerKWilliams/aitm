==== Monitoring

[quote, Wikpedia,https://en.wikipedia.org/wiki/Telemetry]
Telemetry is an automated communications process by which measurements are made and other data collected at remote or inaccessible points and transmitted to receiving equipment for monitoring. The word is derived from Greek roots: 'tele' = remote, and 'metron' = measure.

Computers run in large data centers, where physical access to them is tightly controlled. Therefore, we need telemetry to manage them, more typically called monitoring.

===== Monitoring techniques
But how does one “observe” computing infrastructure? Clearly, sitting in the data center (assuming you could get in) and looking at the faceplates of servers will not convey much useful information. Monitoring tools are the software that watches the software (and systems more broadly).

NOTE: Monitoring systems are similar to source control systems in that they are a critical point at which metadata diverges from the actual system under management.

A variety of techniques are used to monitor computing infrastructure. Typically these involve communication over a network with the device being managed. Often, the network traffic is over the same network carrying the primary traffic of the computers. Sometimes, however, there is a distinct "out of band" network for management traffic.

A classic, simple monitoring tool will simply observe a computing node, perhaps by “pinging” it periodically, and will raise an alert if the node does not respond within an expected time frame. If you are working through this course's labs, you will see an example of this.

More broadly, these tools provide a variety of mechanisms for monitoring and controlling operational IT systems, often through small software “agents” deployed to computing platforms. These agents may monitor processes and their return codes, performance metrics (e.g. memory and CPU utilization), events raised through various channels, network availability, log file contents (e.g. with standing filters for messages indicating problems), interactions with other elements in the IT infrastructure, and more.

All of this data may then forwarded to a central console and integrated, with the objective of supporting the organization’s service level agreements in priority order.
Enterprise monitoring tools are notorious for requiring agents (small, continuously-running programs) on servers; while some things can be detected without such agents, having software running on a given computer still provides the richest data. Since licensing is often agent-based, this gets expensive.

===== Aggregation and operations centers

It is not possible for a Level 1 24 x 7 operations team to access and understand the myriads of element managers and specialized monitoring tools present in the large IT environment. Instead, these teams rely on aggregators of various kinds to provide an integrated view into the complexity. These aggregators may focus on raw status events, or specifically on performance aspects related either to the elements or to logical transactions flowing across them. They may incorporate dependencies from configuration management to provide a true “business view” into the event streams. This is directly analogous to the concept of andon board from Lean practices, or the idea of “information radiator” from Agile principles (*** cite AA)

A monitoring console may present a rich set of information to an operator. Too rich, in fact, as systems become large. For this reason, monitoring tools are often linked directly to ticketing systems; on certain conditions, a ticket is created and assigned to a team or individual.

Enabling a monitoring console to auto-create tickets however needs to be carefully considered and designed. A notorious scenario is the “ticket storm,” where a monitoring system creates multiple (perhaps thousands) of tickets, all essentially in response to the same condition. Event de-duplication starts to become an essential capability, which leads to distinguishing the monitoring system from the event management system.

===== Understanding business impact
At the intersection of event aggregation and operations centers is the need to understand business impact. It is not, for example, always obvious what a server is being used for.

This may be surprising to new students, and perhaps those with experience in smaller organizations. However, in many large “traditional” IT environments, where the operations team is distant from the development organization, it is not necessarily easy to determine what a given hardware or software resource is doing or why it is there.

Clearly, this is unacceptable in terms of security, value management, and any number of other concerns. However, from the start of distributed computing, the question “what is on that server?” has been all too frequent in large IT shops.

In mature organizations, this may be documented in a Configuration Management Database or System (CMDB/CMS). Such a system might start by simply listing the servers and their applications:

[cols="2*", options="header"]
|====
| Application |Server
| Quadrex  |SRV0001
| PL-Q  |SRV0002
| Quadrex |DBSRV001
| TimeTrak |SRV0003
| HR-Portal |SRV0003
| _etc_ | _etc_
|====

(Imagine the above list, 25,000 rows long.)

This is a start, but still doesn't tell us enough. A more elaborate mapping might include business unit and contact:

[cols="4*", options="header"]
|====
|Org|Contact |Application |Server
|Logistics|Mary Smith | Quadrex  |SRV0001
|Finance |Aparna Chaudry |PL-Q  |SRV0002
|Logistics |Mary Smith | Quadrex |DBSRV001
|Human Resources |William Jones |TimeTrak |SRV0003
|Human Resources |William Jones |HR-Portal |SRV0003
| _etc_| _etc_|_etc_ | _etc_
|====

The above lists are very simple examples of what can be extensive record-keeping. But the key user story is implied: if we can't ping SRV0001, we know that the Quadrex application supporting Logistics is at risk, and we should contact Mary Smith ASAP, if she hasn't already contacted us. (Sometimes, the user community calls right away; in other cases, they may not, and proactively contacting them is a positive and important step.)


===== State, configuration, and discovery

Closely related to monitoring is the concept of discovery. While monitoring focuses on the activity of a given system, discovery is focused on state. In particular, discovery tools Answering this question has resulting in a significant market for so-called “discovery” tools, which seek to profile the server and understand its installed software, processing patterns and communications with an objective of assigning it to some management construct, such as a high level IT application or service.

****
*What is the future of discovery tools?*
There is some evidence that the increasing automation of IT infrastructures will mitigate this (which?) problem. As the software pipeline becomes increasingly automated, tracking software from development through production becomes easier.
****

[quote, Wikipedia,https://en.wikipedia.org/wiki/State_(computer_science)]
In computer science and automata theory, the state of a digital logic circuit or computer program is a technical term for all the stored information, at a given instant in time, to which the circuit or program has access.[1] The output of a digital circuit or computer program at any time is completely determined by its current inputs and its state.

In all of IT (whether “infrastructure” or “applications” there is a particular concern with managing state. IT systems are remarkably fragile. One incorrect bit of information - a “0” instead of a “1” - can completely alter a system’s behavior, to the detriment of business operations depending on it.
Therefore, any development of information technology - starting with the initial definition of the computing platform - depends on robust management of state.
￼

The following are examples of state:

* The name of a particular server
* The network address of that server
* The software installed on that server, in terms of the exact version and bits that comprise it.

State also has more transient connotations:

* The current processes listed in the process table
* The memory allocated to each process
* The current users logged into the system

It is therefore not possible to make blanket statements like “we need to manage state.” Computing devices go through myriads of state changes with every cycle of their internal clock. (Analog and quantum computing are out of scope for this book.)

The primary question in managing state is “what matters”? What aspects of the system need to persist, in a reliable and reproducible manner? Two related concepts have emerged in the past decade: policy-based management and promise theory.

In brief, policy-based management and promise theory focus on “what,” not “how.” This may also be described as “declarative,” not “imperative” (see Sidebar).

****
*A simple example of “declarative” vs “imperative”*

Declarative: “Get me a gallon of milk.”

Imperative: “Go out the door, take a right, take a left, go into the building with a big ‘SA’ on it, go in to the last aisle, take a left, go to the third case and take the first container on the fourth shelf from the bottom. Give money to the cashier and bring the container back home.”
****

In configuring infrastructure, scripting is in general considered “imperative” while advanced infrastructure automation frameworks are built using a “declarative,” policy-based approach.

 For example, the following shell script and Chef recipe do the same thing:
 create example - can’t find anything decent on web
 Use directory creation example with failure of script b/c not idempotent

In terms of OODA, determining and tracking the state of a system under investigation is one of the fundamental tools. With current practices, for example, this might mean examining the configuration management/provisioning system to determine how the basic operating system was built.
